%
\section{Konstruktion af simplex-metoden}
% --------------------------------------------------------------
%
For at finde den mest optimale løsning kan der udvælges en basal løsning og undersøges, hvorvidt der findes andre løsninger, som er bedre, end den valgte. 
Hvis der ikke findes bedre løsninger, er den valgte basale løsning den optimale løsning. 
Eftersom der jævnfør afsnit \ref{julieerlakker} optimeres for en konveks funktion over en konveks mængde, %Det er det med at et lokal minimum er et globalt minimum xD
vil den lokale optimale løsning være en global optimal løsning. 
%
%Når der undersøges om der findes et nabo punkt, hvis løsning er bedre, så er det ikke nødvendigt at undersøge i retningen, som leder ud af den basal mulige område.
%
% Definition 3.1 i bert
\begin{defn}{}{}
Lad $\textbf{x}$ være et element af et polyeder $\mathcal{P}$.
En vektor $\textbf{d}\in \R^n$ siges at være en  \textbf{mulig retning} fra $\textbf{x}$, hvis der eksisterer en positiv skalar $\theta$, således at $\textbf{x}+\theta \textbf{d}\in \mathcal{P}$.
\end{defn}
\noindent
%
På figur \ref{fig:julieersmuuuuuk}, ses vektorerne $\mathbf{x}_1, \mathbf{x}_2$ og $\mathbf{x}_3$ i polyederet $\mathcal{P}$, samt tilhørende forskellige mulige retninger, som stadig er indholdt i $\mathcal{P}$.
%
\input{fig/tikz/simplex/mulig_retning}
%
%
%
% Definition 3.2 
\begin{defn}{}{}
Lad $\mathbf{x}$ være en basal løsning til en basis matrix $B$ og tilhørende objektfunktionen $c_B=[ c_{B(1)},c_{B(2)}, \cdots , c_{B(m)} ]^T.$
For hver $j$'te basale retning defineres den \textbf{reducerede objektfunktion} $c_j^*$ som
\begin{align*}
c_j^* = c_j - \mathbf{c}_B^T B^{-1}A_j.
\end{align*} 
% Nogle kalder den den reducerede omkostning.
%
\end{defn}
\noindent
%
Fisk
%
% Sætning 3.1 i Bertsimas 
% Ikke skrevet rigtig endnu 
%\begin{thm}{}{}
%Lad $\mathbf{x}$ være en basal mulig løsning, til en basis matrix $B$ og lad $\mathbf{c}^*$ være den reducerede objektfunktion. 
%\begin{enumerate}[label = (\alph*)]
%\item Hvis $\mathbf{c}^* \geq 0$, så er $\mathbf{x}$ optimal.
%\item Hvis $\mathbf{x}$ er optimal og ikke-degenereret, så er $\mathbf{c}^* \geq 0$.
%\end{enumerate}
%\end{thm}

\begin{thm}{}{}
Lad $\mathbf{x}$ være en basal mulig løsning, til en basis matrix $B$ og lad $\mathbf{c}^*$ være den reducerede objektfunktion. 
\begin{enumerate}[label = (\alph*)]
\item Hvis $\mathbf{c}^* \geq 0$, så er $\mathbf{x}$ optimal.
\item Hvis $\mathbf{x}$ er optimal og ikke-degenereret, så er $\mathbf{c}^* \geq 0$.
\end{enumerate}
\end{thm}
%
\begin{proof}
\begin{enumerate}[label = (\alph*)]
\item Antag, at $\bar{\textbf{c}} \geq \textbf{0}$. Lad $\textbf{y}$ være en arbitrær mulig løsning, og $\textbf{d} = \textbf{y} - \textbf{x}$. Grundet de mulige løsninger, er $\textbf{A} \textbf{x} = \textbf{A} \textbf{y} = \textbf{b}$, hvormed $\textbf{A} \textbf{d} = \textbf{0}$. Sidstnævnte kan skrives som 
$$\textbf{B} \textbf{d}_B + \sum_{i \in N} \textbf{A}_i d_i = \textbf{0},$$
hvor $N$ er mængden af indekser, der svarer til de ikke-basale variabler i basen. %wtf fatter slet ikke det her lol
$\textbf{B}$ er invertibel, hvormed det haves, at 
$$\textbf{d}_B = -\sum_{i \in N} \textbf{B}^{-1} \textbf{A}_i d_i ,$$
og
$$\textbf{c}' \textbf{d} = \textbf{c}'_B \textbf{d}_B + \sum_{i \in N} c_i d_i = \sum_{i \in N} (c_i - \textbf{c}'_B \textbf{B}^{-1} \textbf{A}_i) d_i = \sum_{i \in N} \bar{c}_i d_i .$$
%nanananana fatter intet, hvorfor har vi dette med? 
For ethvert ikke-basalt indeks $i \in N$ må der eksistere et $x_i = 0$, samt et $y_i \geq 0$, da $\textbf{y}$ er en mulig løsning. 
Deraf haves, at $d_i \geq 0$ og $\bar{c}_i d_i \geq 0$ for alle $i \in N$. 
Dermed er $\textbf{c}' (\textbf{y} - \textbf{x} = \textbf{c}' \textbf{d} \geq 0$, og $\textbf{x}$ er en optimal løsning, eftersom $\textbf{y}$ er en arbitrær mulig løsning. 
%
%
\item Antag, at $x$ er en ikke-degenereret basal mulig løsning, og at $\bar{c}_j < 0$ for et $j$. Eftersom den reducerede objektfunktion for en basal variabel altid er $0$, må $x_j$ være en ikke-basal variabel, og $\bar{c}_j$ må være ændringshastigheden for objektfunktionen i den $j$'te basale retning. 
%Wtf er ovenstående for noget lort
Da $\textbf{x}$ ikke er degenereret, er den $j$'te basale retning en mulig retning for mindskelse af objektfunktionen. 
Der findes dermed mulige løsninger, hvis objektfunktion har lavere værdi end $\textbf{x}$'s, ved at bevæge sig i denne retning, hvormed $\textbf{x}$ ikke er optimal. 
\end{enumerate}
\end{proof}
%
%
% Definition 3.3 i Bertsimas
\begin{defn}{}{}
En basis matrix $B$ er optimal, hvis
\begin{enumerate}[label = (\alph*)]
\item $B^-1 \mathbf{b} \geq \mathbf{0}$, og
\item ${\mathbf{c}^*}^T = \mathbf{c}^T - \mathbf{c}_B^T B^{-1} A \geq \mathbf{0}^T$.
\end{enumerate}
\end{defn}
\noindent
%
%
Det skal nu vises, hvordan simplex-metoden kan udvikles ud fra geometriske repræsentationer.
Altså, hvordan der udvikles en metode til at gå fra en profitabel basal mulig løsning til en bedre basal mulig løsning.
Det antages indtil videre, at løsningen er ikke-degenereret.
Antag, at der stås i punktet for den basale mulige løsning $\mathbf{x}$ og at den reducerede objektfunktion $c_j$ er udregnet, hvis alle variable i denne er ikke-negative, haves det fra sætning (3.1 i bertsimas), at løsningen er den optimale, og der behøves derfor ikke yderligere skridt.
Såfremt dette ikke er tilfældet, altså når en ikke-basal variabel $x_j$ i den reducerede objektfunktion er negativ, ville en bedre løsning potentielt forefindes i retningen $\mathbf{d}$, givet ved  $d_j=1$, $d_i=0$ for $i \neq B(1),\ldots,B(m),j$ og $\mathbf{d}_b=-B^{-1}A_j$.
Hvis der foregår en bevægelse i retning $\mathbf{d}$, bliver $x_j$ positiv, imens de øvrige ikke-basale variable forbliver $0$.
Denne bevægelse kan beskrives med $\mathbf{x}+\theta \mathbf{d}$. 
Da værdien af objektfunktionen er aftagende i retning $\mathbf{d}$, er det ønskværdigt at følge $\mathbf{d}$, så langt som muligt med henblik på at ende i punktet $\mathbf{x}+\theta^{*} \mathbf{d}$, hvilket kan udtrykkes som 
$\theta^{*}=\text{max} \theta \geq 0 \mid \mathbf{x}+\theta \mathbf{d} \in \mathcal{P}$.
Forandringen i objektfunktionen er således $\theta^{*}\mathbf{c}^T \mathbf{d}=\theta^{*}c_j$.
Der skal herfra udledes en formel for $\theta^{*}$.
Givet at $A\mathbf{d}=\mathbf{0}$, følger det, at $A(\mathbf{x}+\theta \mathbf{d})=A\mathbf{x}=\mathbf{b}$ for alle $\theta$.
%and the equality constraints will never be violated. hvordan følger det af foregående??
Det gælder således, at $\mathbf{x}+\theta\mathbf{d}$ er en mulig løsning i alle tilfælde, hvor ingen af $\mathbf{d}$'s komponenter er negative.
%usikker på om det er ds komponenter i ovenstående
Der findes således et tilfælde, hvor $\mathbf{d} \geq \mathbf{0}$, hvorfor $\mathbf{x} + \theta \mathbf{d} \geq \mathbf{0}$ for $\forall\theta \geq 0$. 
Det gælder derfor, at $\mathbf{x} + \theta \mathbf{d} $ altid er en mulig løsning, hvormed $\theta^*=\infty$. \\
Såfremt et index i $\mathbf{d}$, $d_i<0$ kan begrænsningen $x_i + \theta d_i \geq 0$ omskrives til $\theta \leq \frac-{x_i}{d_i}$, som skal opfyldes for alle $i$, hvorom det gælder, at $d_i<0$.
Derfor følger det, at $\theta*=\text{min}_{i \mid d_i<0}(-\frac{x_i}{d_i}$.
Da $x_i$ ikke er en basal variabel 
%her kommer det jeg virkeligt ikke fatter umiddelbart
% 
% Afsnit 3.2 i bertsimas
% Sætning 3.2 
\begin{thm}{}{}
\begin{enumerate}[label = (\alph*)]
\item Søjlerne $A_{B(i)}$, $i \neq \mathcal{L}$ og $A_j$ er lineært uafhængige, og 
\item Vektoren $\mathbf{y}=x+ \theta \mathbf{d}$ er en basal mulig løsning til basis matricen $B$. % (associeret med??) 
\end{enumerate}
\end{thm}
%
\begin{proof}
(a) hvis vektorene $\mathbf{A}_{B(i)}$,$i=1,\ldots,m$ er linieært afhængige så eksisterer der koeficienter $\lambda_1,\ldots \lambda_m$ hvor ikke alle af disse er $0$, altså løsninger udover den trivielle løsning, hvorom det gælder:
$ \sum_{i=1}^m \lambda_i \mathbf{A}_{B(i)}=\mathbf{0} $ det følger derfor at $\sum_{i=1}^m \lambda_i B^{-1}\mathbf{A}_{B(i)}=\mathbf{0}$ og vektorene $B^{-1}\mathbf{A}_{B(i)}$ er ligeledes lineært uafhængige. Det skal nu vises at dette ikke er tilfældet, ved at vise at vektorene $B^{-1}\mathbf{A}_{B(i},i\neq \mathcal{L}$ og $B^{-1}\mathbf{A}_j$ er lineært uafhængige.
Jævnfør sætning (den om invers og identitetsmatricen) er $B^{-1}B=i$.
Da $A_{B(i)}$ er den $i$'te søjle af $B$ gælder derfor at vektorene $B^{-1}\mathbf{A}_{B(i},i\neq \mathcal{L}$  er alle enhedsvektorene med undtagelse af den $\mathcal{L}$'te enhedsvektor. 
Vektorene er således lineært uafhængige og deres  $\mathcal{L}$'te komponent er $0$.
Jævnfør sætning (igen en af dem med inverse tror) $B^{-1}\mathbf{A}_j=-\mathbf{d}_B$.(B er stadigt en funktion her men gives intet index, WTF?) 
Den $\mathcal{L}$'te indgang $-d_{B(\mathcal{L})}\neq 0$ ud fra definitionen af $\mathcal{L}$.
Derfor må det gælde at $B^{-1} \mathbf{A}_j$ er lineært uafhængige fra enhedsvektorene $B^{-1}\mathbf{A}_{B(i},i\neq \mathcal{L}$ \\
(b) lad $\mathbf{y}\geq \mathbf{0}$, $A\mathbf{y}=\mathbf{b}$ og $y_i=0$ for $i \neq B(1),\ldots,B(m)$. Søjerne $\mathbf{A}_{B(1)},\ldots,\mathbf{A}_{B(m)}$ er i (a) vist liniæert uafhængige. Det følger derfor at $y$ er en basal mulig løsning associeret med basismatricen $B$.
\end{proof}
\\
%
% Sætning 3.3
% Ikke skrevet rigtig endnu 
%\begin{thm}{}{}
%\begin{enumerate}[label = (\alph*)]
%\item fisk
%\item fisk
%\end{enumerate}
%\end{thm}
%%
%\begin{proof}
%fisk
%\end{proof}
%\\
%%
%fisk
\input{incl/main/simplex_metoden/thm_3.3}