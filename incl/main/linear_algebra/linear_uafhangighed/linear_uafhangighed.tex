\section{Lineær uafhængighed}
% 1.7
En mængde af vektorer i $\R^n$ kan være \textit{lineært afhængige} eller \textit{lineært uafhængige}.
%
%En mængde af vektorer er \textbf{lineært afhængig}, hvis alle vektorerne kan skrives som en linearkombination af de andre vektorer. 
%Hvis ikke alle vektorerne kan skrives som en linearkombination af de andre vektorer, så er mængden \textbf{lineært uafhængige}. 
%Det kan derfor siges, at hvis mindst en af vektorerne afhænger af de andre vektorer, er samlingen lineært afhængig, og dette leder frem til følgende definition. 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
\begin{defn}{Lineær afhængighed og lineær uafhængighed}{}
%En mængde af vektorer $\mathbf{u}_1, \mathbf{u}_2, \ldots , \mathbf{u}_k \in \R^n$ kaldes lineært uafhængige, hvis der eksisterer skalarer $x_1, x_2, \ldots , x_k$, sådan at ligningen 
En mængde af vektorer $\mathbf{u}_1, \mathbf{u}_2, \ldots , \mathbf{u}_k \in \R^n$ kaldes \textbf{lineært uafhængige}, hvis der eksisterer skalarer $x_1, x_2, \ldots , x_k$, sådan at ligningen 
\begin{align*}
x_1\mathbf{u}_1 + x_2\mathbf{u}_2 + \cdots + x_k \mathbf{u}_k = \mathbf{0}, 
\end{align*}
kun har den trivielle løsning $x_1 = x_2 = \cdots = x_k = 0$.
\\
Ellers kaldes vektorerne \textbf{lineært afhængige}.
\end{defn}  
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Eksempel
\begin{eks}\label{fisk}
Givet
\begin{align*}
\text{span}\{\S_1 \} =
\left\{ 
\begin{bmatrix}
           2 \\
           1 \\
\end{bmatrix} 
,
\begin{bmatrix}
           1 \\
           3 \\
\end{bmatrix}
\right\},
\end{align*}
opstilles ligningen  
%
\begin{align*}
x_1 
\begin{bmatrix}
           2 \\
           1 \\
\end{bmatrix} 
+ x_2
\begin{bmatrix}
           1 \\
           3 \\
\end{bmatrix}
= \mathbf{0}.
\end{align*}
%
Denne ligning er kun opfyldt, hvis $x_1=x_2=0$. 
Eftersom det kun eksisterer den triviele løsnig, er vektorerne i $\S_1$ lineært uafhængige.
\\\\
Havde spannet været 
\begin{align*}
\text{span}\{\S_2 \} =
\left\{ 
\begin{bmatrix}
           2 \\
           1 \\
\end{bmatrix} 
,
\begin{bmatrix}
           4 \\
           2 \\
\end{bmatrix}
\right\},
\end{align*}
%
ville ligningen 
%
\begin{align*}
x_1 
\begin{bmatrix}
           2 \\
           1 \\
\end{bmatrix} 
+ x_2
\begin{bmatrix}
           4 \\
           2 \\
\end{bmatrix}
= \mathbf{0}
\end{align*}
%
være opfyldt, hvis $x_1=x_2=0$ eller $x_1=-2$ og $x_2=1$. 
Vektorerne i $\S_2$ er derfor lineært afhængige, fordi der er flere løsninger end den trivielle.
%
\end{eks}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%I eksempel \ref{lineu} viste en af måderne hvorpå man kunne bestemme om et bestemt set $\S$ var lineært afhængigt eller uafhængigt. 
%Der findes flere måder at bestemme dette på. Denne følgende sætning viser en lignende teknik til bestemmelse heraf. 
%
Egenskaber af lineær uafhængighed kan udtrykkes på forskellige måder, hvilket leder til \ref{thm:mxlinuaf}.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Brug \ref når i refererer til sætninger, i stedet for at skrive "følgende sætning.." (syns jeg). 
\begin{thm}{}{mxlinuaf}
%
Lad $A$ være en $m \times n$ matrix.
Følgende udsagn er ækvivalente:
%
\begin{enumerate}[label=(\alph*)]
\item $A$'s søjler er lineært uafhængige. 
\item Ligningssystemet $A\mathbf{x}=\mathbf{b}$ har højst en løsning $\forall \mathbf{b} \in \R^m$.
\item $\text{null}(A)=0$.
\item $\text{rang}(A)=n$.
\item Søjlerne i $A$ på reduceret trappeform er standardvektoren i $\R^m$.
\item $A\mathbf{x}=\mathbf{0}$ har kun løsningen $\mathbf{x}=\mathbf{0}$.                                                                                                                                                                                                                                                                                                                         
\item Der er pivot-indgange i hver søjle i $A$. 
\end{enumerate}
%
\end{thm}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{proof}
%Forudsagt? Mangler der en ref til et tidligere bevis her, eller er I spåkællinger? (nedenstående) 
Lad $A$ være en $m \times n$ matrix.
Det er åbenlyst, at (a) og (f), samt (f) og (g) er ækvivalente, så for at færdiggøre beviset for denne sætning skal det vises, at (b) $\rightarrow$ (c), (c) $\rightarrow$ (d), (d) $\rightarrow$ (e), (e) $\rightarrow$ (f) og (f) $\rightarrow$ (b).
\\\\
%Er de pile korrekt notation, eller skal vi bruge \equiv? (ovenstående)
% (b) $\rightarrow$ (c). 
Idet $\mathbf{0}$ er en løsning til $A\mathbf{x}=\mathbf{b}$, så forudsætter (b), at $A\mathbf{x}=\mathbf{b}$ ikke har ikke-nul løsninger, da den generelle løsning ingen frie variable har. 
Eftersom antallet af frie variable er $0$, og $\text{null}(A)$ dermed er nul, er (b) $\rightarrow$ (c) hermed bevist. 
\\\\
%Der skal ikke komma efter "idet". 
% (c) $\rightarrow$ (d).
Idet $\text{rang}(A)+\text{null}(A)=n$, så er (c) $\rightarrow$ (d) bevist.
\\\\
%
% (d) $\rightarrow$ (e).
Hvis $\text{rang}(A)=n$, så indeholder enhver søjle i $A$ en pivotindgang, hvormed den reducerede trappeform udelukkende indeholder standardvektorer. 
Disse standardvektorer er nødvendigvis forskellige, da hver søjle indeholder den første ikke-nul indgang i hver række. 
Dette beviser (d) $\rightarrow$ (e).
%
\\\\
% (e) $\rightarrow$ (f).
Lad $R$ være den reducerede trappeform af matricen $A$. Hvis søjlerne i $R$ er forskellige standardvektorer i $\R^m$, så er $R= [ \mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n]$. 
Det er givet, at den eneste løsning til $R\mathbf{x}=\mathbf{0}$ er $\mathbf{0}$, og idet $A\mathbf{x}=\mathbf{0}$ er ækvivalent med $R\mathbf{x}=\mathbf{0}$, så er $\mathbf{0}$ eneste løsning til A$\mathbf{x}=\mathbf{0}$. 
Dette beviser (e) $\rightarrow$ (f).
\\\\
%
% (f) $\rightarrow$ (b).
Lad $\mathbf{u}$ og $\mathbf{v}$ være løsninger til $A\mathbf{x}=\mathbf{b}$.
% Sætning 1.3
Jævnfør sæ \ref{thm:mxvpro} følger det, at $A(\mathbf{u}-\mathbf{v})=A\mathbf{u}-A\mathbf{v}=\mathbf{b}-\mathbf{b}=\mathbf{0}$, hvilket tilsvarer $\mathbf{u}-\mathbf{v}=\mathbf{0}
\rightarrow  
\mathbf{u} =\mathbf{v}$. Dette beviser (f) $\rightarrow$ (b).
\\\\
%
Lad $\mathbf{b}=\mathbf{0}$. 
Ligningssystemet $A\mathbf{x}=\mathbf{b}$ har altid løsningen $\mathbf{x}=\mathbf{0}$, der derfor er den eneste løsning til $A\mathbf{x}=\mathbf{0}$, hvilket beviser (b) $\rightarrow$ (e).
%
\end{proof}
\\
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Ligeså findes der en række egenskaber vedrørende mængder, der er lineært afhængige, som er belyst i \ref{thm:inspektion}.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{thm}{}{inspektion}
%
Vektorerne $\mathbf{u}_1,\mathbf{u}_2, \ldots ,\mathbf{u}_k \in\R^n$ er lineært afhængige, 
hvis og kun hvis $\mathbf{u}_1=\mathbf{0}$, eller hvis der findes $i \geq 2$, således $\mathbf{u}_i \in \text{span} \{ \mathbf{u}_1,\mathbf{u}_2, \ldots ,\mathbf{u}_{i-1} \}$.
%
\end{thm}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{proof}
% 
Antag, at $\mathbf{u}_1 \neq \mathbf{0}$, da løsningen ellers er triviel. 
Der findes skalarer, hvor variablerne ikke alle er $0$, således at
% 
\begin{align*}
x_1 \mathbf{u}_1 + x_2 \mathbf{u}_2 + \ldots + x_k \mathbf{u}_k = \mathbf{0}.
\end{align*}
%
Lad $i$ være det største index $i$, hvor $x_i \neq 0$, sådan at 
\begin{align*}
x_1 \mathbf{u}_1 + x_2 \mathbf{u}_2 + \ldots + x_i \mathbf{u}_i = \mathbf{0}.
\end{align*}
%Synes det ovenstående lyder lidt mærkeligt (det med i, i og x_i). 
Bemærk, at $i\geq2$, da udtrykket ellers ville blive reduceret til
%
\begin{align*}
x_1\textbf{u}_1=\mathbf{0},
\end{align*}
%
hvilket går imod tidligere antagelser.
Løses udtrykket i forhold til $\mathbf{u}_i$, ses det, at
\begin{align*}
\mathbf{u}_i = - \frac{x_1}{x_i} \mathbf{u}_1 - \frac{x_2}{x_i} \mathbf{u}_2 - \cdots - \frac{x_{i-1}}{x_i} \mathbf{u}_{i-1}.
\end{align*}
Hvilket vil sige, at 
\begin{align*}
\mathbf{u}_i \in \text{span}\{ \mathbf{u}_1,\mathbf{u}_2, \ldots ,\mathbf{u}_{i-1} \}.
\end{align*}
%
Antag nu, at $\mathbf{u}_i \in \text{span}\{ \mathbf{u}_1,\mathbf{u}_2, \ldots ,\mathbf{u}_{i-1} \}$ for $i \geq 2$.
Da $\text{span}\{ \mathbf{u}_1,\mathbf{u}_2, \ldots ,\mathbf{u}_{i-1} \}$ er mængden af alle linearkombinationer af vektorerne $\mathbf{u}_1,\mathbf{u}_2, \ldots ,\mathbf{u}_{i-1}$, eksisterer der skalarer $x_1, x_2, \ldots x_{i-1}$, således at
%
\begin{align*}
\textbf{u}_i=x_1\mathbf{u}_1 + x_2 \mathbf{u}_2 + \cdots + x_{i-1}\mathbf{u}_{i-1}.
\end{align*}
%
For $k \geq i$ haves, at $\mathbf{u}_1,\mathbf{u}_2, \ldots ,\mathbf{u}_k$ er lineært afhængig, da 
\begin{align*}
\textbf{u}_1 -x_1\textbf{u}_1 - x_2\textbf{u}_2 - \cdots - x_{i-1}\textbf{u}_{i-1} = \textbf{0}
\end{align*} 
er en løsning anderledes fra den trivielle løsning, hvor $x_1, x_2,\ldots,x_k=0$.
%
\end{proof}
\\
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Det kan i nogle tilfælde afgøres ved inspektion, hvorvidt en mængde vektorer $\mathbf{u},\mathbf{u}_2,\ldots ,\mathbf{u}_k$ er lineært afhængige. 
Jævnfør \ref{thm:inspektion} er mængden af vektorer afhængige, hvis $\mathbf{0}$ er en af dem, hvis to af vektorerne er parallelle, eller hvis der er flere vektorer end hver vektors antal af koordinater. 
Med udgangspunkt i \ref{fisk} ses det, at de to vektorer i $\S_2$ er parallelle og derfor er lineært afhængige. 
\\\\
Hvis det ikke ved hjælp af inspektion kan afgøres, om en matrix er lineært afhængig eller lineært uafhængig, kan det undersøges ved at opstille totalmatricen af mængden af vektorer, hvor matricen derefter via rækkeopperationer laves til trappeform. 
Hvis der er pivotindgange i alle søjlerne, er mængden af vektorer lineært uafhængige. Hvis der er frie variable, er mængden af vektorer derimod lineært afhængige.
\\\\
%
% Eksempel
% Virker stadig ufærdigt, vi snakker om det. 
\begin{eks}\label{lineu}
Lad
\begin{align*}
\S &= \left\{
\begin{bmatrix}
           4 \\
           1 \\
           1 \\
\end{bmatrix}
,
\begin{bmatrix}
           -3 \\
           0 \\
           1 \\
\end{bmatrix}
,
\begin{bmatrix}
           1 \\
           -2 \\
           1 \\
\end{bmatrix}
\right\}.
\end{align*}
\noindent
%
Hvis vektorerne er lineært uafhængige er den trivielle løsning den eneste løsning til
%
\begin{align*}
x_1
\begin{bmatrix}
           4 \\
           1 \\
           1 \\
\end{bmatrix}
+ x_2
\begin{bmatrix}
           -3 \\
           0 \\
           1 \\
\end{bmatrix}
+ x_3
\begin{bmatrix}
           1 \\
           -2 \\
           1 \\
\end{bmatrix}
=\mathbf{0}.
\end{align*}
%
Ved hjælp af rækkeoperationerne omskrives totalmatricen til reduceret trappeform:
%
\noindent
\begin{align*}
A=
\begin{blockarray}{cccc}
x_1 & x_2 & x_3 & b \\
\begin{block}{[ccc|c]}
4 & -3 & 1 & 0\\
1 & 0 & -2 & 0 \\
1 & 1 & 1 & 0 \\
\end{block}
\end{blockarray}
\xrightarrow[R_2 \rightarrow R_2+(-1R_1)]{R_1 \rightarrow \frac{1}{4}R_1} 
\begin{blockarray}{cccc}
x_1 & x_2 & x_3 & b \\ 
\begin{block}{[ccc|c]}
1 & -\frac{3}{4} & \frac{1}{4} & 0\\
0 & \frac{3}{4} & -\frac{9}{4} & 0 \\
1 & 1 & 1 & 0 \\
\end{block}
\end{blockarray} \\
\xrightarrow[R_2 \rightarrow \frac{4}{3} \times R_2]{R_3 \rightarrow R_3+(-1R_1)} 
\begin{blockarray}{cccc}
x_1 & x_2 & x_3 & b \\
\begin{block}{[ccc|c]}
1 & -\frac{3}{4} & \frac{1}{4} & 0\\
0 & 1 & -3 & 0\\
0 & \frac{7}{4} & \frac{3}{4} & 0 \\
\end{block}
\end{blockarray}
\xrightarrow[R_3 \rightarrow \frac{1}{6} \times R_3]{R_3 \rightarrow R_3+(-\frac{7}{4}R_2)} 
\begin{blockarray}{cccc}
x_1 & x_2 & x_3 & b \\
\begin{block}{[ccc|c]}
\hlight{1} & \frac{-3}{4} & \frac{1}{4} & 0\\
0 & \hlight{1} & -3 & 0\\
0 & 0 & \hlight{1} & 0 \\
\end{block}
\end{blockarray} \\
\xrightarrow[R_1 \rightarrow R_1+(-\frac{1}{4}R_3)]{R_2 \rightarrow R_2+3R_3} 
\begin{blockarray}{cccc}
x_1 & x_2 & x_3 & b \\
\begin{block}{[ccc|c]}
1 & \frac{-3}{4} & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0 \\
\end{block}
\end{blockarray} 
\xrightarrow[]{R_1 \rightarrow R_1+\frac{3}{4}R_2} 
\begin{blockarray}{cccc}
x_1 & x_2 & x_3 & b \\
\begin{block}{[ccc|c]}
\hlight{1} & 0 & 0 & 0\\
0 & \hlight{1} & 0 & 0\\
0 & 0 & \hlight{1} & 0 \\
\end{block}
\end{blockarray} \\
\end{align*}
%
Eftersom totalmatricen ikke har nogle frie variable, har den kun en løsning og er derfor lineært uafhængig.
\\
\\
\noindent
Lad nu
%
\begin{align*}
\S &= \left\{
\begin{bmatrix}
           4 \\
           1 \\
           -2 \\
\end{bmatrix}
,
\begin{bmatrix}
           -3 \\
           0 \\
           1 \\
\end{bmatrix}
,
\begin{bmatrix}
           1 \\
           -2 \\
           1 \\
\end{bmatrix}
\right\}.
\end{align*}
%
Nu afgøres, om den trivielle løsning er den eneste løsning til linearkombinationen.
%
\begin{align*}
x_1+
\begin{bmatrix}
           4 \\
           1 \\
           -2 \\
\end{bmatrix}
+ x_2
\begin{bmatrix}
          -3 \\
           0 \\
           1 \\
\end{bmatrix}
+ x_3
\begin{bmatrix}
           1 \\
           -2 \\
           1 \\
\end{bmatrix}
=0.
\end{align*}
%
Ved hjælp af rækkeoperationerne omskrives totalmatricen til reduceret trappeform:
%
\begin{align*}
A=&
\begin{blockarray}{cccc}
x_1 & x_2 & x_3 & b \\
\begin{block}{[ccc|c]}
4 & -3 & 1 & 0\\
1 & 0 & -2 & 0 \\
-2 & 1 & 1 & 0 \\
\end{block}
\end{blockarray}\\
\xrightarrow[R_2\rightarrow R_2+(-4R_1)]{R_1 \leftrightarrow R_2}&
\begin{blockarray}{cccc}
x_1 & x_2 & x_3 & b \\
\begin{block}{[ccc|c]}
1 & 0 & -2 & 0\\
0 & -3 & 9 & 0\\
-2 & 1 & 1 & 0 \\
\end{block}
\end{blockarray}\\
\xrightarrow[R_3 \rightarrow R_3+\frac{1}{3}R_2]{R_3 \rightarrow R_3+2R_1}&
\begin{blockarray}{cccc}
x_1 & x_2 & x_3 & b \\
\begin{block}{[ccc|c]}
\hlight{1} & 0 & -2 & 0\\
0 & \hlight{-3} & 9 & 0 \\
0 & 0 & 0 & 0 \\
\end{block}
\end{blockarray}
\end{align*}
%
Dette betyder, at der findes en ikke-triviel linearkombination, som resulterer i nulvektoren. Derfor har vi fundet mindst én linearkombination, som ikke er $0$, og derfor er det bevist, at vektormængden er lineært afhængig.
\end{eks}
%Der bliver stadig skrevet "sæt" rigtig meget.. :-( 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% SKAL MÅSKE FLYTTES
I forbindelse med en given mængde vektorer kan færre vektorer udvælges for at få en mere effektiv beskrivelse af spannet. Dette gøres ved at finde spannet af pivotsøjlerne i koefficientmatricen.
\\\\
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Eksempel
\begin{eks}
Givet 
\begin{align*}
\S &= \left\{
\begin{bmatrix}
           1 \\
           2 \\
           0 \\
\end{bmatrix}
,
\begin{bmatrix}
           -1 \\
           2 \\
           4 \\
\end{bmatrix}
,
\begin{bmatrix}
           3 \\
           0 \\
           1 \\
\end{bmatrix}
,
\begin{bmatrix}
           1 \\
           0 \\
           5 \\
\end{bmatrix}
\right\},
\end{align*}
%
opstilles koefficientmatricen, som omdannes til reduceret trappeform. 
%
\begin{align*}
&\begin{blockarray}{ccccc}
\begin{block}{[cccc]c}
  1 & -1 & 3 & 1 \\
  2 & 2 & 0 & 0 \\
  0 & 4 & 1 & 5 \\
\end{block}
\end{blockarray}
%
& \xrightarrow{R_2 \rightarrow R_2-2R_1}&
\begin{blockarray}{ccccc}
\begin{block}{[cccc]c}
  1 & -1 & 3 & 1 \\
  0 & 4 & -6 & -2 \\
  0 & 4 & 1 & 5 \\
\end{block}
\end{blockarray} \\
%
\xrightarrow{R_3 \rightarrow R_3-R_2}&
\begin{blockarray}{ccccc}
\begin{block}{[cccc]c}
  \hlight{1} & -1 & 3 & 1 \\
  0 & \hlight{4} & -6 & -2 \\
  0 & 0 & \hlight{7} & 7 \\
\end{block}
\end{blockarray} 
%
&\xrightarrow{R_3 \rightarrow 1/7 R_3}&
\begin{blockarray}{ccccc}
\begin{block}{[cccc]c}
  1 & -1 & 3 & 1 \\
  0 & 4 & -6 & -2 \\
  0 & 0 & 1 & 1 \\
\end{block}
\end{blockarray} \\
%
\xrightarrow{R_2 \rightarrow R_2+6R_3}&
\begin{blockarray}{ccccc}
\begin{block}{[cccc]c}
  1 & -1 & 3 & 1 \\
  0 & 4 & 0 & 4 \\
  0 & 0 & 1 & 1 \\
\end{block}
\end{blockarray} 
%
&\xrightarrow{R_1 \rightarrow R_1-3R_3}&
\begin{blockarray}{ccccc}
\begin{block}{[cccc]c}
  1 & -1 & 0 & -2 \\
  0 & 4 & 0 & 4 \\
  0 & 0 & 1 & 1 \\
\end{block}
\end{blockarray} \\
%
\xrightarrow{R_2 \rightarrow 1/4R_2}&
\begin{blockarray}{ccccc}
\begin{block}{[cccc]c}
  1 & -1 & 0 & -2 \\
  0 & 1 & 0 & 1 \\
  0 & 0 & 1 & 1 \\
\end{block}
\end{blockarray} 
%
&\xrightarrow{R_1 \rightarrow R_1+R_2}&
\begin{blockarray}{ccccc}
\begin{block}{[cccc]c}
  \hlight{1} & 0 & 0 & -1 \\
  0 & \hlight{1} & 0 & 1 \\
  0 & 0 & \hlight{1} & 1 \\
\end{block}
\end{blockarray}.
%
\end{align*}
%
Hermed er den sidste vektor i spannet en linearkombination af de andre vektorer:
%
  \begin{align*}
         -1 \begin{bmatrix}
           1 \\
           2 \\
           0 \\
         \end{bmatrix}
         +1
         \begin{bmatrix}
           -1 \\
           2 \\
           4 \\
         \end{bmatrix}
          +1
         \begin{bmatrix}
           3 \\
           0 \\
           1 \\
         \end{bmatrix}
         =
         \begin{bmatrix}
           1 \\
           0 \\
           5 \\
         \end{bmatrix}.
  \end{align*} 
%
Dermed kan spannet af $\S$ vælges til
%
\begin{align*}
\S &= \left\{
\begin{bmatrix}
           1 \\
           2 \\
           0 \\
\end{bmatrix}
,
\begin{bmatrix}
           -1 \\
           2 \\
           4 \\
\end{bmatrix}
,
\begin{bmatrix}
           3 \\
           0 \\
           1 \\
\end{bmatrix}
\right\}.
\end{align*}
\end{eks}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{eks}\label{lineu}
%Lad et sæt af vektorer være
%\begin{align*}
%\S &= \left\{
%\begin{bmatrix}
%           2 \\
%           1 \\
%\end{bmatrix}
%,
%\begin{bmatrix}
%           3 \\
%           2 \\
%\end{bmatrix}
%\right\}.
%\end{align*}
%\noindent
%Bestem om vektorerne er lineært uafhængige ved ligningen, altså hvis $x_1$ og $x_2 = 0$
%\begin{align*}
%x_1+
%\begin{bmatrix}
%           2 \\
%           1 \\
%\end{bmatrix}
%+ x_2
%\begin{bmatrix}
%           3 \\
%           2 \\
%\end{bmatrix}
%=0 
%\end{align*}
%%
%Ligningerne udskrives
%\noindent
%\begin{align*}
%2x_1+3x_2=0\\
%x_1+2x_2=0\\
%\\
%x_1+3/2x_2=0\\
%x_1+2x_2=0\\
%\\
%x_1+2x_2=0 - x_1+3/2x_2=0\\
%1/2c_2=0\\
%c_2=0\\
%c_1=0
%\end{align*}
%%
%Da $c_1$ og $c_2 = 0$, så er sættet $\S$ lineært uafhængigt.
%\\
%\\
%\noindent
%Lad et sæt af vektorer være 
%%
%\begin{align*}
%\S &= \left\{
%\begin{bmatrix}
%           2 \\
%           1 \\
%\end{bmatrix}
%,
%\begin{bmatrix}
%           3 \\
%           2 \\
%\end{bmatrix}
%,
%\begin{bmatrix}
%           1 \\
%           2 \\
%\end{bmatrix}
%\right\}.
%\end{align*}
%\noindent
%Bestem om vektorerne er lineært afhængige ved ligningen, altså hvis $x_1$, $x_2$ og $x_3 \neq 0$
%\begin{align*}
%x_1+
%\begin{bmatrix}
%           2 \\
%           1 \\
%\end{bmatrix}
%+ x_2
%\begin{bmatrix}
%           3 \\
%           2 \\
%\end{bmatrix}
%+ x_3
%\begin{bmatrix}
%           1 \\
%           2 \\
%\end{bmatrix}
%=0.
%\end{align*}
%%
%Ligningerne udskrives 
%%
%\begin{align*}
%2x_1+3x_2+c_3=0\\
%x_1+2x_2+2c_3=0 
%\end{align*}
%\noindent
%Nu findes en linearkombination, som ikke er nul. 
%Vi sætter tilfældigt $x_3 = -1$ og forsøger om ligningen kan løses
%%
%\begin{align*}
%2x_1+3x_2-1=0\\
%x_1+2x_2-2=0\\
%\\
%2x_1+3x_2-1=0\\
%2x_1+4x_2-4=0\\
%\\	 
%2x_1+3x_2-1=0 - 2x_1+4x_2-4=0 \\
%-x_2+3=0\\
%-x_2=-3\\
%x_2=3\\
%\\
%x_1+6+2\\
%x_1+4=0\\
%x_1=-4
%\end{align*}
%%
%Linearkombinationen er derfor
%%
%\begin{align*}
%-4+
%\begin{bmatrix}
%           2 \\
%           1 \\
%\end{bmatrix}
%+ 3
%\begin{bmatrix}
%           3 \\
%           2 \\
%\end{bmatrix}
%+ -1
%\begin{bmatrix}
%           1 \\
%           2 \\
%\end{bmatrix}
%=0
%\end{align*}
%%
%Derfor har vi minimum fundet én linearkombination som er ikke-nul, som giver os nulvektoren, og derfor er det bevist at vektorsættet lineært afhængig.
%\end{eks}
%

